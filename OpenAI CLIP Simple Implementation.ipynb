{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was in January of 2021 that **OpenAI** announced two new models: **DALL-E** and **CLIP**, both **multi-modality** models connecting **texts and images** in some way. In this article we are going to implement CLIP model from scratch in **PyTorch**. OpenAI has open-sourced some of the code relating to CLIP model but I found it intimidating and it was far from something short and simple. I also came across a good tutorial inspired by CLIP model on Keras code examples and I translated some parts of it into PyTorch to build this tutorial totally with our beloved PyTorch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Meterials:\n",
    "- [Understanding OpenAI’s CLIP model](https://medium.com/@paluchasz/understanding-openais-clip-model-6b52bade3fa3)\n",
    "- [Learning Transferable Visual Models From Natural Language Supervision paper](https://arxiv.org/abs/2103.00020)\n",
    "- [CLIP Paper Explained Easily in 3 Levels of Detail](https://medium.com/one-minute-machine-learning/clip-paper-explained-easily-in-3-levels-of-detail-61959814ad13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does CLIP do? Why is it fun?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CLIP: Contrastive Language-Image Pre-training, is a model for calculating how good a given image and a given text caption fit together.\n",
    "- In training, it tries to maximize the “cosine similarity” between correct image-text embedding pairs , and minimize the similarity scores between all incorrect pairs.\n",
    "- It aligns text and image embeddings by first projecting them on a common embedding space then minimizing contrastive loss\n",
    "- In inference, it calculates the similarity scores between the embedding of a given image with embeddings from a given list of possible texts, and picks the text with the highest similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Learning Transferable Visual Models From Natural Language Supervision paper](https://arxiv.org/abs/2103.00020), OpenAI introduces their new model which is called **CLIP**, for **Contrastive Language-Image Pre-training**. In a nutshell, this model learns the relationship between a whole sentence and the image it describes; in a sense that when the model is trained, given an input sentence it will be able to retrieve the most related images corresponding to that sentence. The important thing here is that it is trained on full sentences instead of single classes like car, dog, etc. The intuition is that when trained on whole sentences, the model can learn a lot more things and finds some pattern between images and texts.\n",
    "They also show that when this model is trained on a huge dataset of images and their corresponding texts, it can also act as a classifier too. I encourage you to study the paper to learn more about this exciting model and their astonishing results on benchmarking datasets . To mention just one, CLIP model trained with this strategy classifies ImageNet better than those SOTA models trained on the ImageNet itself optimized for the only task of classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a **teaser** (!), let's see what the final model that we will build in this article from scratch is capable of: given a query (raw text) like \"a boy jumping with skateboard\" or \"a girl jumping from swing\", the model will retrieve the most relevant images:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title_img](./images/teaser.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some more outputs:\n",
    "\n",
    "![](./images/dogs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notable Successors and Inspired Models\n",
    "- **[ALIGN](https://arxiv.org/abs/2102.05918):** Google’s adaptation of CLIP, using larger datasets and simpler architectures.\n",
    "- **[Florence](https://arxiv.org/abs/2111.11432):** A model by Microsoft, focusing on domain generality and vision-language understanding.\n",
    "- **[Flamingo](https://arxiv.org/abs/2204.14198):** DeepMind’s extension for video-text tasks.\n",
    "- **DALL·E:** Leveraging CLIP-like representations for text-to-image generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.862689,
     "end_time": "2021-04-05T08:01:49.835804",
     "exception": false,
     "start_time": "2021-04-05T08:01:45.973115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "import albumentations as A\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012732,
     "end_time": "2021-04-05T08:01:51.324144",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.311412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A note on config and CFG: I wrote the codes with python scripts and then converted it into a Jupyter Notebook. So, in case of python scripts, config is a normal python file where I put all the hyperparameters and in the case of Jupyter Notebook, its a class defined in the beginning of the notebook to keep all the hyperparameters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.377383,
     "end_time": "2021-04-05T08:01:51.714313",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.336930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug = False\n",
    "    image_path = \"C:/Moein/AI/Datasets/Flicker-8k/Images\"\n",
    "    captions_path = \"C:/Moein/AI/Datasets/Flicker-8k\"\n",
    "    batch_size = 32\n",
    "    num_workers = 4\n",
    "    head_lr = 1e-3\n",
    "    image_encoder_lr = 1e-4\n",
    "    text_encoder_lr = 1e-5\n",
    "    weight_decay = 1e-3\n",
    "    patience = 1\n",
    "    factor = 0.8\n",
    "    epochs = 4\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_name = 'resnet50'\n",
    "    image_embedding = 2048\n",
    "    text_encoder_model = \"distilbert-base-uncased\"\n",
    "    text_embedding = 768\n",
    "    text_tokenizer = \"distilbert-base-uncased\"\n",
    "    max_length = 200\n",
    "\n",
    "    pretrained = True # for both image encoder and text encoder\n",
    "    trainable = True # for both image encoder and text encoder\n",
    "    temperature = 1.0\n",
    "\n",
    "    # image size\n",
    "    size = 224\n",
    "\n",
    "    # for projection head; used for both image and text encoders\n",
    "    num_projection_layers = 1\n",
    "    projection_dim = 256 \n",
    "    dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012746,
     "end_time": "2021-04-05T08:01:51.740070",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.727324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.023459,
     "end_time": "2021-04-05T08:01:51.776328",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.752869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012817,
     "end_time": "2021-04-05T08:01:51.802043",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.789226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the tittle image of this article, we need to encode both images and their describing texts. So, the dataset needs to **return both images and texts**. Of course we are not going to feed raw text to our text encoder! We will use **DistilBERT** model (which is smaller than BERT but performs nearly as well as BERT) from **HuggingFace** library as our text encoder; so, we need to **tokenize** the sentences (captions) with DistilBERT tokenizer and then feed the token ids (input_ids) and the attention masks to DistilBERT. Therefore, the dataset needs to take care of the tokenization as well. Below you can see the dataset's code. Below that I'll explain the most important things that is happening in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **\\_\\_init\\_\\_** we receive a tokenizer object which is actually a HuggingFace tokinzer; this tokenizer will be loaded when running the model. We are padding and truncating the captions to a specified max_length. In the **\\_\\_getitem\\_\\_** we will first load an encoded caption which is a dictionary with keys input_ids and attention_mask, make tensors out of its values and after that we will load the corresponding image, transform and augment it (if there is any!) and then we make it a tensor and put it in the dictionary with \"image\" as the key. Finally we put the raw text of the caption with the key \"caption\" in the dictionary only for visualization purposes. \n",
    "\n",
    "I did not use additional data augmentations but you can add them if you want to improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.025532,
     "end_time": "2021-04-05T08:01:51.840523",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.814991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
    "        \"\"\"\n",
    "        image_filenames and cpations must have the same length; so, if there are\n",
    "        multiple captions for each image, the image_filenames must have repetitive\n",
    "        file names \n",
    "        \"\"\"\n",
    "\n",
    "        self.image_filenames = image_filenames\n",
    "        self.captions = list(captions)\n",
    "        self.encoded_captions = tokenizer(\n",
    "            list(captions), padding=True, truncation=True, max_length=CFG.max_length\n",
    "        )\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(values[idx])\n",
    "            for key, values in self.encoded_captions.items()\n",
    "        }\n",
    "\n",
    "        image = cv2.imread(f\"{CFG.image_path}/{self.image_filenames[idx]}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transforms(image=image)['image']\n",
    "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        item['caption'] = self.captions[idx]\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "\n",
    "\n",
    "def get_transforms(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012853,
     "end_time": "2021-04-05T08:01:51.866433",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.853580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Image Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image encoder code is straight forward. I'm using PyTorch Image Models library (timm) here which makes a lot of different image models available from ResNets to EfficientNets and many more. Here we will use a ResNet50 as our image encoder. You can easily use torchvision library to use ResNets if you don't want to install a new library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code encodes each image to a fixed size vector with the size of the model's output channels (in case of ResNet50 the vector size will be **2048**). This is the output after the nn.AdaptiveAvgPool2d() layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.027706,
     "end_time": "2021-04-05T08:01:51.907283",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.879577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode images to a fixed size vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
    "        )\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned before, I'll use DistilBERT as the text encoder. Like its bigger brother BERT, two special tokens will be added to the actual input tokens: **CLS** and **SEP** which mark the start and end of a sentence. To grab the whole representation of a sentence (as the related BERT and DistilBERT papers point out) we use the final representations of the CLS token and we hope that this representation captures the overall meaning of the sentence (caption). Thinking it in this way, it is similar to what we did to images and converted them into a fixed size vector.\n",
    "\n",
    "In the case of DistilBERT (and also BERT) the output hidden representation for each token is a vector with size **768**. So, the whole caption will be encoded in the CLS token representation whose size is 768."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.027706,
     "end_time": "2021-04-05T08:01:51.907283",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.879577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = DistilBertModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.model = DistilBertModel(config=DistilBertConfig())\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "        # we are using the CLS token hidden representation as the sentence's embedding\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state[:, self.target_token_idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used [Keras code example implementation](https://keras.io/examples/nlp/nl_image_search/) of projection head to write the following in PyTorch.\n",
    "Now that we have encoded both our images and texts into fixed size vectors (2048 for image and 768 for text) we need to bring (project) them into a **new world** (!) with **similar dimensions** for both images and texts in order to be able to compare them and push apart the non-relevant image and texts and pull together those that match. So, the following code will bring the 2048 and 768 dimensional vectors into a 256 (projection_dim) dimensional world, where we can **compare** them.\n",
    "\n",
    "\"embedding_dim\" is the size of the input vector (2048 for images and 768 for texts) and \"projection_dim\" is the the size of the output vector which will be 256 for our case. For understanding the details of this part you can refer to the CLIP paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.027706,
     "end_time": "2021-04-05T08:01:51.907283",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.879577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=CFG.projection_dim,\n",
    "        dropout=CFG.dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012961,
     "end_time": "2021-04-05T08:01:51.933336",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.920375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is where all the fun happens! I'll also talk about the loss function here. I translated some of the code from Keras code examples into PyTorch for writing this part. Take a look at the code and then read the explanation below this code block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the previous modules that we built to implement the main model. The \\_\\_init\\_\\_ function is self-explanatory. In the forward function, we first encode the images and texts separately into fixed size vectors (with different dimensionalities). After that, using separate projection modules we project them to that shared world (space) that I talked about previously. Here the encodings will become of similar shape (256 in our case). After that we will compute the loss. Again I recommend reading CLIP paper to get it better but I'll try my best to explain this part.\n",
    "\n",
    "In **Linear Algebra**, one common way to measure if two vectors are of similar characteristics (they are like each other) is to calculate their **dot product** (multiplying the matching entries and take the sum of them); if the final number is big, they are alike and if it is small they are not (relatively speaking)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! What I just said is the most important thing to have in mind to understand this loss function. Let's continue. We talked about two vectors, but, what do we have here? We have image_embeddings, a matrix with shape (batch_size, 256) and text_embeddings with shape (batch_size, 256). Easy enough! it means we have two groups of vectors instead of two single vectors. How do we measure how similar two groups of vectors (two matrices) are to each other? Again, with dot product (@ operator in PyTorch does the dot product or matrix multiplication in this case). To be able to multiply these two matrices together, we transpose the second one. Okay, we get a matrix with shape (batch_size, batch_size) which we will call logits. (temperature is equal to 1.0 in our case, so, it does not make a difference. You can play with it and see what difference it makes. Also look at the paper to see why it is here!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you are still with me! If not it's okay, just review the code and check their shapes. Now that we have our logits, we need targets. I need to say that there is a more straight forward way to obtain targets but I had to do this for our case (I'll talk about why in a next paragraph)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider what we hope that this model learns: **we want it to learn \"similar representations (vectors)\" for a given image and the caption describing it. Meaning that either we give it an image or the text describing it, we want it to produce same 256 sized vectors for both.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the cell below this code block for the continue of the explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.025366,
     "end_time": "2021-04-05T08:01:51.972338",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.946972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature=CFG.temperature,\n",
    "        image_embedding=CFG.image_embedding,\n",
    "        text_embedding=CFG.text_embedding,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Getting Image and Text Features\n",
    "        image_features = self.image_encoder(batch[\"image\"])\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        # Getting Image and Text Embeddings (with same dimension)\n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "\n",
    "        # Calculating the Loss\n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "        images_similarity = image_embeddings @ image_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "        targets = F.softmax(\n",
    "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        )\n",
    "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in the best case scenario, text_embeddings and image_embedding matricies should be the same because they are describing similar things. Let's think now: if this happens, what would the logits matrix be like? Let's see with a simple example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple Example\n",
    "\n",
    "batch_size = 4\n",
    "dim = 256\n",
    "embeddings = torch.randn(batch_size, dim)\n",
    "out = embeddings @ embeddings.T\n",
    "print(F.softmax(out, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So logits, in the best case, will be a matrix that if we take its softmax, will have 1.0s in the diagonal (An identity matrix to call it with fancy words!). As the loss function's job is to make model's predictions similar to targets (at least in most cases!), we want such a matrix as our target. That's the reason why we are calculating images_similarity and texts_similarity matrices in the code block above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got our targets matrix, we will use simple cross entropy to calculate the actual loss. I've written the full matrix form of cross entropy as a function which you can see in the bottom of the code block. Okay! We are done! Wasn't it simple?! Alright, you can ignore the next paragraph but if you are curious, there is an important note in that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here's why I didn't use a simpler approach**: I need to admit that there's a simpler way to calculate this loss in PyTorch; by doing this: nn.CrossEntropyLoss()(logits, torch.arange(batch_size)). Why I did not use it here? For 2 reasons. 1- The dataset we are using has multiple captions for a single image; so, there is the possibility that two identical images with their similar captions exist in a batch (it is rare but it can happen). Taking the loss with this easier method will ignore this possibility and the model learns to pull apart two representations (assume them different)  that are actually the same. Obviously, we don't want this to happen so I calculated the whole target matrix in a way that takes care of these edge cases. 2- Doing it the way I did, gave me a better understanding of what is happening in this loss function; so, I thought it would give you a better intuition as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## A Closer Look at the Loss Calculation\n",
    "\n",
    "Note that this loss computation is not the original loss calculation provided by OpenAI. THe original idea was to make sure that in mini-batches, we would try to maximise cosine similarity for each image-text pairs and minimize other wrong image-text pairs. This results in a target of Identity matrix.\n",
    "\n",
    "But, some images or texts can be similar themselves. For example- text 1: \"A dog\", text 2: \"A bull-dog\" and text 3: \"A cat\". We see that text 1 and 2 are not similar but 1 and 3 have close meanings. Similarly, in images there are samples that are closer to each other. So, if we want to account for this, the target can't be strictly identidy matrix but rather we need to _measure self-similarity among the image/text embeddings_. That's what happened in the code above. So, the target is not necessarily is indentity matrix but rather depends on the text/ image in the mini-batch.\n",
    "\n",
    "\n",
    "### Explanation of the Loss Function\n",
    "\n",
    "1. **Forward Pass**:\n",
    "   - `image_features` and `text_features` are extracted using respective encoders.\n",
    "   - These features are projected to the same embedding space using `ProjectionHead`.\n",
    "\n",
    "2. **Logits Calculation**:\n",
    "   - The logits are computed as the similarity scores between `text_embeddings` and `image_embeddings`, scaled by the inverse of the temperature parameter:\n",
    "     $     \\text{logits}_{ij} = \\frac{\\text{text\\_embeddings}_i \\cdot \\text{image\\_embeddings}_j^\\top}{\\text{temperature}}     $\n",
    "\n",
    "3. **Target Similarity Calculation**:\n",
    "   - `images_similarity` and `texts_similarity` matrices represent the self-similarity within image and text embeddings.\n",
    "   - The target similarity matrix is computed as the softmax of the average of these two similarities, scaled by the temperature:\n",
    "     $     \\text{targets} = \\text{softmax}\\left(\\frac{\\text{images\\_similarity} + \\text{texts\\_similarity}}{2 \\cdot \\text{temperature}}\\right)     $\n",
    "\n",
    "4. **Loss Calculation**:\n",
    "   - Cross-entropy loss is computed for the logits, with target similarity matrix, for both directions (text-to-image and image-to-text).\n",
    "   - Both losses are averaged:\n",
    "     $    \\text{loss} = \\frac{\\text{images\\_loss} + \\text{texts\\_loss}}{2}     $\n",
    "\n",
    "5. **Reduction**:\n",
    "   - The per-sample loss is averaged over the batch.\n",
    "\n",
    "\n",
    "\n",
    "### Differences from the Original CLIP Loss Function\n",
    "\n",
    "The original CLIP model uses **contrastive loss**:\n",
    "- **Logits Calculation**:\n",
    "  $  \\text{logits}_{ij} = \\frac{\\text{text\\_embeddings}_i \\cdot \\text{image\\_embeddings}_j^\\top}{\\text{temperature}}  $\n",
    "- **Targets**: A diagonal matrix of one-hot vectors indicating perfect alignment between the matching text and image embeddings.\n",
    "- **Loss**:\n",
    "  - Cross-entropy loss is computed for the logits, with one-hot targets, for both directions (text-to-image and image-to-text).\n",
    "\n",
    "From the original paper:\n",
    "\n",
    "_Given a batch of N (image, text) pairs, CLIP is trained to predict which of the N × N possible (image, text) pairings across a batch actually occurred. To do this, CLIP learns a multi-modal embedding space by jointly training an image encoder and text encoder to maximise the cosine similarity of the image and text embeddings of the N real pairs in the batch while minimising the cosine similarity of the embeddings of the N² − N incorrect pairings. It optimises a symmetric cross entropy loss over these similarity scores._\n",
    "\n",
    "#### Key Differences:\n",
    "1. **Target Definition**:\n",
    "   - Original CLIP: Uses **one-hot encoding** to define perfect matching between text and image embeddings.\n",
    "   - New Model: Defines **soft targets** using averaged self-similarity matrices of images and texts, making the targets more distributed and allowing for smoother optimization.\n",
    "\n",
    "2. **Loss Design**:\n",
    "   - Original CLIP: Computes cross-entropy directly on the logits with one-hot targets.\n",
    "   - New Model: Incorporates **softmax-normalized self-similarity matrices** as targets, making the loss more dependent on intra-batch relationships.\n",
    "\n",
    "3. **Softness of Targets**:\n",
    "   - The new loss introduces a softer approach to target creation, which might better handle noisy or less-aligned data compared to the strict one-hot targets in the original.\n",
    "\n",
    "4. **Complexity**:\n",
    "   - The new model introduces additional computation for `images_similarity` and `texts_similarity`, making the loss slightly more complex.\n",
    "\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "This new loss function generalizes the contrastive loss by incorporating intra-modal similarities and soft targets. It may improve training stability and robustness in scenarios where image-text alignments are ambiguous or noisy. However, it is computationally heavier than the original CLIP loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP model with original loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLIP model with original loss function\n",
    "\n",
    "class CLIPModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature=CFG.temperature,\n",
    "        image_embedding=CFG.image_embedding,\n",
    "        text_embedding=CFG.text_embedding,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Get Image and Text Features\n",
    "        image_features = self.image_encoder(batch[\"image\"])  # [batch_size, image_embedding]\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )  # [batch_size, text_embedding]\n",
    "        \n",
    "        # Project Features to Same Embedding Space\n",
    "        image_embeddings = self.image_projection(image_features)  # [batch_size, embedding_dim]\n",
    "        text_embeddings = self.text_projection(text_features)  # [batch_size, embedding_dim]\n",
    "\n",
    "        # Normalize Embeddings with l2 norm so that dot product is cosine similarity\n",
    "        image_embeddings = F.normalize(image_embeddings, dim=-1)\n",
    "        text_embeddings = F.normalize(text_embeddings, dim=-1)\n",
    "\n",
    "        # Compute Logits\n",
    "        logits_per_text = text_embeddings @ image_embeddings.T  # [batch_size, batch_size]\n",
    "        logits_per_image = logits_per_text.T  # [batch_size, batch_size]\n",
    "\n",
    "        # Scale by Temperature\n",
    "        logits_per_text /= self.temperature\n",
    "        logits_per_image /= self.temperature\n",
    "\n",
    "        # Create One-Hot Targets\n",
    "        batch_size = logits_per_text.size(0)\n",
    "        targets = torch.arange(batch_size).to(logits_per_text.device)  # [batch_size, 1] Ex- targets = [0, 1, ..., batch_size-1], these are class indices for one-hot encoding\n",
    "\n",
    "        # Compute Loss\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n",
    "        # cross_entropy expects  logits: [batch_s, n_classes] and targets: [batch_s] if the target is a class index\n",
    "        # this means that cross_entropy will internally create one-hot encodings and the target specifies which index to light up\n",
    "        # Ex- target = [0,1,2] will be converted to [[1,0,0], [0,1,0], [0,0,1]] internally\n",
    "        text_loss = F.cross_entropy(logits_per_text, targets)\n",
    "        image_loss = F.cross_entropy(logits_per_image, targets)\n",
    "        loss = (text_loss + image_loss) / 2.0  # Average the two losses\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a [snippet](https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/modeling_clip.py#L1187) from HuggingFace CLIP model:\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "class CLIPModel(CLIPPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config: CLIPConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "\n",
    "        text_config = config.text_config\n",
    "        vision_config = config.vision_config\n",
    "\n",
    "        self.projection_dim = config.projection_dim\n",
    "        self.text_embed_dim = text_config.hidden_size\n",
    "        self.vision_embed_dim = vision_config.hidden_size\n",
    "\n",
    "        text_model = CLIPTextModel._from_config(text_config)\n",
    "        self.text_model = text_model.text_model\n",
    "\n",
    "        vision_model = CLIPVisionModel._from_config(vision_config)\n",
    "        self.vision_model = vision_model.vision_model\n",
    "\n",
    "        self.visual_projection = nn.Linear(self.vision_embed_dim, self.projection_dim, bias=False)\n",
    "        self.text_projection = nn.Linear(self.text_embed_dim, self.projection_dim, bias=False)\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(self.config.logit_scale_init_value))\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: Optional[torch.LongTensor] = None,\n",
    "            pixel_values: Optional[torch.FloatTensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            position_ids: Optional[torch.LongTensor] = None,\n",
    "            return_loss: Optional[bool] = None,\n",
    "            output_attentions: Optional[bool] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            interpolate_pos_encoding: bool = False,\n",
    "            return_dict: Optional[bool] = None,\n",
    "        ) -> Union[Tuple, CLIPOutput]:\n",
    "        \n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            interpolate_pos_encoding=interpolate_pos_encoding,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        text_outputs = self.text_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # get image embedding and project to common space\n",
    "        image_embeds = vision_outputs[1]\n",
    "        image_embeds = self.visual_projection(image_embeds)\n",
    "\n",
    "        # get text embedding and project to common space\n",
    "        text_embeds = text_outputs[1]\n",
    "        text_embeds = self.text_projection(text_embeds)\n",
    "\n",
    "        # normalize projected embeddings\n",
    "        image_embeds = image_embeds / _get_vector_norm(image_embeds)\n",
    "        text_embeds = text_embeds / _get_vector_norm(text_embeds)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp() # Returns a new tensor with the exponential of the elements of the input tensor\n",
    "        logits_per_text = torch.matmul(text_embeds, image_embeds.t().to(text_embeds.device)) * logit_scale.to(\n",
    "            text_embeds.device\n",
    "        )\n",
    "        logits_per_image = logits_per_text.t()\n",
    "\n",
    "        # calculate loss\n",
    "        loss = None\n",
    "        if return_loss:\n",
    "            loss = clip_loss(logits_per_text)\n",
    "        \n",
    "        # return loss with other things\n",
    "        return CLIPOutput(\n",
    "            loss=loss,\n",
    "            logits_per_image=logits_per_image,\n",
    "            logits_per_text=logits_per_text,\n",
    "            text_embeds=text_embeds,\n",
    "            image_embeds=image_embeds,\n",
    "            text_model_output=text_outputs,\n",
    "            vision_model_output=vision_outputs,\n",
    "        )\n",
    "\n",
    "    # the logit matrix is the same for both text and image. we just need to transpose to get one from another\n",
    "    # Here, we only take one logit matrix (logits_per_text) and transpose it to get the other (logits_per_image)\n",
    "    # We could do this more neatly as above but I am keeping the original codes here intact so you can find them in the source repo\n",
    "    # Here, both losses are calculated and averaged\n",
    "    def clip_loss(similarity: torch.Tensor) -> torch.Tensor:\n",
    "        caption_loss = contrastive_loss(similarity)\n",
    "        image_loss = contrastive_loss(similarity.t())\n",
    "        return (caption_loss + image_loss) / 2.0\n",
    "\n",
    "    # simple cross-entropy loss using one-hot encoding class index as targets    \n",
    "    def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:\n",
    "    return nn.functional.cross_entropy(logits, torch.arange(len(logits), device=logits.device))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **original CLIP loss function** is more popular and widely used in practice. Here's why:\n",
    "\n",
    "### Reasons for Popularity of the Original CLIP Loss:\n",
    "1. **Simplicity**:  \n",
    "   - The one-hot target design of the original loss is straightforward and computationally efficient.\n",
    "   - It directly enforces a clear correspondence between text and image embeddings.\n",
    "\n",
    "2. **Effectiveness**:  \n",
    "   - The original CLIP loss has been extensively validated and proven to work well across diverse tasks, including zero-shot learning, image-text retrieval, and transfer learning.\n",
    "\n",
    "3. **Adoption by Large Models**:  \n",
    "   - OpenAI's CLIP has set a standard for vision-language pretraining. Many subsequent models, such as ALIGN and Flamingo, adopt or adapt the original contrastive loss.\n",
    "\n",
    "4. **Efficiency**:  \n",
    "   - The original approach avoids computing and averaging intra-modal similarities, making it computationally lighter for large-scale datasets.\n",
    "\n",
    "\n",
    "### Use Cases for the Modified Loss:\n",
    "The modified loss, which uses soft targets derived from intra-modal similarities, could be more useful in scenarios where:\n",
    "- Data has **noisy or ambiguous image-text alignments**.\n",
    "- There’s a need to capture **intra-batch relationships**, emphasizing the structure within images or texts.\n",
    "- The task benefits from a smoother optimization landscape, e.g., fine-tuning on smaller datasets or niche domains.\n",
    "\n",
    "\n",
    "In summary, the original CLIP loss remains the most popular due to its simplicity, efficiency, and strong performance across standard benchmarks. However, the modified loss might gain traction in specific research or fine-tuning scenarios where soft targets offer an advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013097,
     "end_time": "2021-04-05T08:01:51.998635",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.985538",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original work, both the text and image encoder were trained from scratch.\n",
    "\n",
    "_We train CLIP from scratch without initializing the image encoder with ImageNet weights or the text encoder with pre-trained weights._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some funtions to help us load train and valid dataloaders, our model and then train and evaluate our model on those. There's not much going on here; just simple training loop and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.041512,
     "end_time": "2021-04-05T08:01:52.054352",
     "exception": false,
     "start_time": "2021-04-05T08:01:52.012840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_train_valid_dfs():\n",
    "    dataframe = pd.read_csv(f\"{CFG.captions_path}/captions.csv\")\n",
    "    max_id = dataframe[\"id\"].max() + 1 if not CFG.debug else 100\n",
    "    image_ids = np.arange(0, max_id)\n",
    "    np.random.seed(42)\n",
    "    valid_ids = np.random.choice(\n",
    "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
    "    )\n",
    "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
    "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
    "    return train_dataframe, valid_dataframe\n",
    "\n",
    "\n",
    "def build_loaders(dataframe, tokenizer, mode):\n",
    "    transforms = get_transforms(mode=mode)\n",
    "    dataset = CLIPDataset(\n",
    "        dataframe[\"image\"].values,\n",
    "        dataframe[\"caption\"].values,\n",
    "        tokenizer=tokenizer,\n",
    "        transforms=transforms,\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        num_workers=CFG.num_workers,\n",
    "        shuffle=True if mode == \"train\" else False,\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a handy function to train our model. There's not much happening here; just loading the batches, feeding them to the model and stepping the optimizer and lr_scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.041512,
     "end_time": "2021-04-05T08:01:52.054352",
     "exception": false,
     "start_time": "2021-04-05T08:01:52.012840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
    "        loss = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step == \"batch\":\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
    "    return loss_meter\n",
    "\n",
    "\n",
    "def valid_epoch(model, valid_loader):\n",
    "    loss_meter = AvgMeter()\n",
    "\n",
    "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
    "        loss = model(batch)\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n",
    "    return loss_meter\n",
    "\n",
    "\n",
    "def main():\n",
    "    train_df, valid_df = make_train_valid_dfs()\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "    train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
    "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
    "\n",
    "\n",
    "    model = CLIPModel().to(CFG.device)\n",
    "    params = [\n",
    "        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr},\n",
    "        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n",
    "        {\"params\": itertools.chain(\n",
    "            model.image_projection.parameters(), model.text_projection.parameters()\n",
    "        ), \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay}\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(params, weight_decay=0.)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", patience=CFG.patience, factor=CFG.factor\n",
    "    )\n",
    "    step = \"epoch\"\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(CFG.epochs):\n",
    "        print(f\"Epoch: {epoch + 1}\")\n",
    "        model.train()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = valid_epoch(model, valid_loader)\n",
    "        \n",
    "        if valid_loss.avg < best_loss:\n",
    "            best_loss = valid_loss.avg\n",
    "            torch.save(model.state_dict(), \"best.pt\")\n",
    "            print(\"Saved Best Model!\")\n",
    "        \n",
    "        lr_scheduler.step(valid_loss.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the next cell start training the model. Put the kernel on GPU mode. Every epoch should take about 24 minutes on GPU (even one epoch is enough!). It can take one minute before training actually starts because we are going to encode all the captions once in the train and valid dataset, so please don't stop it! Every thing is working fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 16449.265031,
     "end_time": "2021-04-05T12:36:01.333760",
     "exception": false,
     "start_time": "2021-04-05T08:01:52.068729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! We are done with training the model. Now, we need to do inference which in our case will be giving the model a piece of text and want it to retrieve the most relevant images from an unseen validation (or test) set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Image Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, we are loading the model that we saved after training, feeding it images in validation set and returning the image_embeddings with shape (valid_set_size, 256) and the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings(valid_df, model_path):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
    "    \n",
    "    model = CLIPModel().to(CFG.device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=CFG.device))\n",
    "    model.eval()\n",
    "    \n",
    "    valid_image_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader):\n",
    "            image_features = model.image_encoder(batch[\"image\"].to(CFG.device))\n",
    "            image_embeddings = model.image_projection(image_features)\n",
    "            valid_image_embeddings.append(image_embeddings)\n",
    "    return model, torch.cat(valid_image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, valid_df = make_train_valid_dfs()\n",
    "model, image_embeddings = get_image_embeddings(valid_df, \"best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function does the final task that we wished our model would be capable of: it gets the model, image_embeddings, and a text query. It will display the most relevant images from the validation set! Isn't it amazing? Let's see how it performs after all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.025647,
     "end_time": "2021-04-05T12:36:01.385717",
     "exception": false,
     "start_time": "2021-04-05T12:36:01.360070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_matches(model, image_embeddings, query, image_filenames, n=9):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "    encoded_query = tokenizer([query])\n",
    "    batch = {\n",
    "        key: torch.tensor(values).to(CFG.device)\n",
    "        for key, values in encoded_query.items()\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        text_features = model.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        text_embeddings = model.text_projection(text_features)\n",
    "    \n",
    "    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n",
    "    \n",
    "    values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)\n",
    "    matches = [image_filenames[idx] for idx in indices[::5]]\n",
    "    \n",
    "    _, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    for match, ax in zip(matches, axes.flatten()):\n",
    "        image = cv2.imread(f\"{CFG.image_path}/{match}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(image)\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we use this function. Aaaannnndddd the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_matches(model, \n",
    "             image_embeddings,\n",
    "             query=\"a group of people dancing in a party\",\n",
    "             image_filenames=valid_df['image'].values,\n",
    "             n=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/dance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you have enjoyed this article. Implementing this paper was a really interesting experience for me. I want to thank Khalid Salama for the great Keras code example he provided which inspired me to write something similar in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "Since the release of the original CLIP model by OpenAI, several advancements have been made to improve its performance, usability, and application across various domains. These advancements typically focus on enhancing the model architecture, training methodologies, and domain adaptability. Here are some notable areas of improvement:\n",
    "\n",
    "\n",
    "1. **Architecture Enhancements**\n",
    "   - **Vision Transformers (ViTs):**  \n",
    "      - CLIP originally used ResNet and Vision Transformers for the image encoder. Subsequent research explored deeper or hierarchical ViT architectures to better capture multi-scale image features.\n",
    "      - Models like **Florence** and **ALIGN** introduced improvements in transformer-based encoders for vision and multimodal tasks.\n",
    "   - **Cross-Attention Mechanisms:**  \n",
    "      - Adding cross-attention layers between text and image embeddings to enhance multimodal feature fusion.\n",
    "      - Models like **Flamingo** (by DeepMind) utilize such mechanisms for better image-text alignment.\n",
    "\n",
    "\n",
    "2. **Loss Function Improvements**\n",
    "   - **Decoupled Contrastive Loss:**  \n",
    "      - Separate contrastive losses for different modalities to focus more effectively on their unique characteristics.\n",
    "   - **Fine-Grained Matching:**  \n",
    "      - Loss functions that emphasize fine-grained alignments, e.g., aligning specific objects in an image with corresponding phrases in text.\n",
    "\n",
    "\n",
    "3. **Training on Larger and Diverse Datasets**\n",
    "   - **LAION Datasets:**  \n",
    "      - The LAION project curated massive open datasets (e.g., LAION-400M, LAION-5B) with hundreds of millions of image-text pairs, enabling better generalization and fine-tuning opportunities.\n",
    "   - **Multi-Lingual Training:**  \n",
    "      - Extensions to train on multilingual datasets for broader applicability (e.g., **mCLIP**).\n",
    "   - **Domain-Specific Datasets:**  \n",
    "      - Adaptations of CLIP for domains like healthcare, remote sensing, and gaming by fine-tuning on task-specific datasets.\n",
    "\n",
    "\n",
    "4. **Specialized Variants**\n",
    "   - **OpenCLIP:**  \n",
    "      - An open-source implementation designed for broader experimentation, including support for custom datasets and architectures.\n",
    "   - **CLIPSeg:**  \n",
    "      - A variant of CLIP fine-tuned for segmentation tasks, demonstrating its versatility in image understanding beyond classification and retrieval.\n",
    "   - **CLIPA:**  \n",
    "      - Models designed for **audio-visual alignment**, extending CLIP's functionality to the audio domain.\n",
    "\n",
    "\n",
    "5. **Incorporation of Additional Modalities**\n",
    "   - **Video CLIP:**  \n",
    "      - Adapting CLIP for video tasks by introducing temporal encoders and aligning text with video frames.\n",
    "   - **Text-to-3D Models:**  \n",
    "      - Extensions for creating 3D objects from text, leveraging CLIP’s rich text-image understanding.\n",
    "   - **Audio-CLIP:**  \n",
    "      - Models integrating audio encoders for tasks like audio-caption alignment and audio-visual retrieval.\n",
    "\n",
    "\n",
    "6. **Efficiency and Optimization**\n",
    "   - **Efficient Training Strategies:**  \n",
    "      - Techniques like **progressive training**, **low-rank adaptation (LoRA)**, and **adapter layers** reduce computational costs while maintaining performance.\n",
    "   - **Distillation and Pruning:**  \n",
    "      - Distilling CLIP into smaller, faster models for deployment on edge devices and mobile platforms.\n",
    "\n",
    "\n",
    "7. **Applications and Domain Adaptation**\n",
    "   - **Zero-Shot Transfer Learning:**  \n",
    "      - Improvements in zero-shot generalization through better pretraining strategies.\n",
    "   - **Prompt Engineering:**  \n",
    "      - Exploring optimal ways to design text prompts to enhance performance across tasks.\n",
    "   - **Robustness to Domain Shifts:**  \n",
    "      - Fine-tuning to improve robustness in domains like medical imaging, satellite data, or artistic styles.\n",
    "\n",
    "\n",
    "8. **Alignment and Fairness**\n",
    "   - **Bias Mitigation:**  \n",
    "      - Researchers have analyzed biases in the original CLIP model and proposed strategies for debiasing, including better dataset curation and bias-regularized training.\n",
    "   - **Ethical Considerations:**  \n",
    "      - Addressing misuse and ensuring CLIP is used responsibly, particularly in tasks involving sensitive or high-stakes decision-making.\n",
    "\n",
    "\n",
    "9. **Cross-Domain Applications**\n",
    "   - **Text-to-Image Generation (Diffusion Models):**  \n",
    "      - Using CLIP as a scoring mechanism for guiding text-to-image generation models like **DALL·E**, **Stable Diffusion**, and **MidJourney**.\n",
    "   - **Image Editing:**  \n",
    "      - CLIP is widely used for text-guided image manipulation and inpainting.\n",
    "   - **Robotics and Navigation:**  \n",
    "      - CLIP has been adapted for vision-based navigation and robotics applications, where understanding contextual information is crucial.\n",
    "\n",
    "\n",
    "These advancements reflect the versatility of the CLIP framework and its adaptability to a wide range of tasks, making it a cornerstone of multimodal AI research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
